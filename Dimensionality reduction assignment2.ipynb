{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea9219ec-86b9-4d75-80c4-42bd91d08492",
   "metadata": {},
   "source": [
    "# Question - 1\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ff2f1-11ae-4ed0-8aff-ddb8cdeb5423",
   "metadata": {},
   "source": [
    "A projection is a linear transformation that maps points from one space onto a lower-dimensional subspace. In the context of Principal Component Analysis (PCA), projection is used to reduce the dimensionality of a dataset by projecting it onto a lower-dimensional subspace while preserving as much variance as possible.\n",
    "\n",
    "Here's how projection is used in PCA:\n",
    "\n",
    "1. Compute Principal Components: PCA calculates the principal components, which are the orthogonal axes that capture the directions of maximum variance in the original dataset.\n",
    "\n",
    "2. Select Components: The number of principal components chosen determines the dimensionality of the subspace onto which the data will be projected. Typically, the components are ranked by the amount of variance they explain, and a certain number of components are selected to retain a desired amount of variance (e.g., 95%).\n",
    "\n",
    "3. Projection: The selected principal components form a new basis for the data. Each data point is projected onto this new basis, resulting in a lower-dimensional representation of the original data.\n",
    "\n",
    "4. Dimensionality Reduction: The projected data lies in a subspace spanned by the selected principal components. By discarding the least important components (those with lower variance), PCA effectively reduces the dimensionality of the data while retaining as much information as possible.\n",
    "\n",
    "5. Reconstruction: If needed, the reduced-dimensional data can be transformed back to the original high-dimensional space using the inverse transformation, known as reconstruction. However, the reconstructed data may not perfectly match the original data due to information loss during dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759cb4de-9103-4beb-ab67-e9a64ca4d71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9fdca0-8945-4a86-b95e-49f494def418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a99e6-6255-438b-8dca-b5e1384bc92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dc71aaf-96ea-4ea4-8515-65313283dfa4",
   "metadata": {},
   "source": [
    "## Question - 2\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad697f-b242-46b5-8a88-7db46deaefdc",
   "metadata": {},
   "source": [
    "\n",
    "The optimization problem in Principal Component Analysis (PCA) involves finding the directions (principal components) in the feature space that maximize the variance of the projected data. PCA aims to achieve dimensionality reduction by projecting the original high-dimensional data onto a lower-dimensional subspace while preserving as much variance as possible.\n",
    "\n",
    "Here's how the optimization problem in PCA works and what it aims to achieve:\n",
    "\n",
    "1. Variance Maximization: PCA seeks to find the directions (principal components) along which the variance of the data is maximized. These principal components represent the axes that capture the most significant variations in the dataset.\n",
    "\n",
    "2. Eigenvalue Decomposition or Singular Value Decomposition (SVD): The optimization problem in PCA is typically solved using eigenvalue decomposition or singular value decomposition (SVD) of the covariance matrix of the data.\n",
    "\n",
    "3. Eigenvalue Decomposition: In eigenvalue decomposition, the covariance matrix of the data is decomposed into its eigenvectors and eigenvalues. The eigenvectors represent the principal components, and the eigenvalues represent the amount of variance explained by each principal component.\n",
    "\n",
    "4. Singular Value Decomposition (SVD): In SVD, the data matrix is decomposed into three matrices: U,Σ,V^T. The columns of U represent the left singular vectors (equivalent to eigenvectors), and the diagonal elements of Σ represent the singular values (square roots of eigenvalues), which indicate the amount of variance explained by each principal component.\n",
    "\n",
    "5. Dimensionality Reduction: Once the principal components are obtained, PCA selects a subset of them that captures a desired amount of variance (e.g., 95%). These principal components form a new basis for the data, onto which the original high-dimensional data is projected. By discarding the least important components (those with lower variance), PCA effectively reduces the dimensionality of the data.\n",
    "\n",
    "6. Objective Function: The optimization problem in PCA can be formulated as maximizing an objective function, such as the total variance of the projected data or the proportion of variance explained by each principal component. Mathematically, this can be expressed as maximizing:\n",
    "\n",
    "\n",
    "# var(y)/var(x) = var(Wt . x) / var(x)\n",
    " \n",
    "\n",
    "where X is the original data matrix, W is the matrix of principal components, and Y is the projected data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5775c88-0d8d-492f-9a58-b570304c7602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8878436-08cb-412a-b204-71cd85b2887e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c3b171a-147e-492d-afd8-4b27ee511c86",
   "metadata": {},
   "source": [
    "# Question - 3\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc319a2-c791-4c90-8be2-7c1215f39694",
   "metadata": {},
   "source": [
    "The relationship between covariance matrices and Principal Component Analysis (PCA) is fundamental to understanding how PCA works.\n",
    "\n",
    "1. Covariance Matrix:\n",
    "\n",
    "* The covariance matrix is a symmetric matrix that quantifies the covariance (or correlation) between pairs of features in a dataset. It provides information about the linear relationship between variables.\n",
    "\n",
    "* For a dataset with n features, the covariance matrix C is an n×n matrix, where the element at row i and column j represents the covariance between the i-th and j-th features.\n",
    "\n",
    "\n",
    "2. PCA and Covariance Matrix:\n",
    "\n",
    "* PCA is a dimensionality reduction technique that aims to find the directions (principal components) in the feature space that maximize the variance of the projected data.\n",
    "\n",
    "* The principal components are the eigenvectors of the covariance matrix of the data. The eigenvectors represent the directions of maximum variance in the dataset, and the corresponding eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "* Mathematically, PCA involves computing the covariance matrix of the data and then performing eigenvalue decomposition (or singular value decomposition) on this covariance matrix to obtain the principal components.\n",
    "\n",
    "3. Eigenvalue Decomposition of Covariance Matrix:\n",
    "\n",
    "* The eigenvalue decomposition of the covariance matrix yields the principal components and their corresponding eigenvalues. The eigenvectors represent the directions (axes) along which the data varies the most, and the eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "* The principal components are ordered by their corresponding eigenvalues, with the first principal component capturing the most variance, the second capturing the second most variance, and so on.\n",
    "\n",
    "4. Dimensionality Reduction:\n",
    "\n",
    "* PCA selects a subset of the principal components that captures a desired amount of variance (e.g., 95%). These principal components form a new basis for the data onto which the original high-dimensional data is projected.\n",
    "\n",
    "* By projecting the data onto a lower-dimensional subspace spanned by the selected principal components, PCA effectively reduces the dimensionality of the data while preserving as much information as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225dc60-40c3-4e71-930d-76cfd358e5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afbe0fa-e56f-45f9-bcf2-f9ced9a47bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6521b23-cab0-40c6-8a66-fc0a8e7ab046",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748c4d51-914d-4493-a8e5-14680505656c",
   "metadata": {},
   "source": [
    "# Question - 4\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd95fc6-fad6-4dfc-bc1c-31a395ffa766",
   "metadata": {},
   "source": [
    "\n",
    "The choice of the number of principal components in PCA has a significant impact on the performance and effectiveness of the dimensionality reduction process. Here's how the choice of the number of principal components affects PCA performance:\n",
    "\n",
    "1. Explained Variance:\n",
    "\n",
    "* Each principal component captures a certain amount of variance in the original dataset. The cumulative explained variance of the principal components increases as more components are added.\n",
    "\n",
    "* Selecting a larger number of principal components results in a higher proportion of total variance being explained by the reduced-dimensional representation of the data. This can lead to better preservation of information and potentially higher performance in downstream tasks.\n",
    "\n",
    "2. Dimensionality Reduction:\n",
    "\n",
    "* PCA aims to reduce the dimensionality of the dataset while preserving as much variance as possible. Choosing a larger number of principal components retains more information from the original data, resulting in a higher-dimensional reduced-dimensional representation.\n",
    "\n",
    "* However, selecting too many principal components may lead to overfitting or increased computational complexity, especially if the added components capture noise or irrelevant variance in the data.\n",
    "\n",
    "3. Overfitting and Generalization:\n",
    "\n",
    "* Selecting too many principal components can lead to overfitting, where the reduced-dimensional representation captures noise or irrelevant patterns from the training data. This may result in poor generalization performance on unseen data.\n",
    "\n",
    "* On the other hand, selecting too few principal components may lead to underfitting, where the reduced-dimensional representation fails to capture important information from the data, resulting in decreased performance.\n",
    "\n",
    "4. Computational Efficiency:\n",
    "\n",
    "* Choosing a smaller number of principal components reduces the computational complexity of the PCA algorithm, as fewer eigenvectors need to be computed and fewer dimensions need to be projected onto. This can lead to faster training and inference times, especially for large datasets.\n",
    "\n",
    "5. Interpretability:\n",
    "\n",
    "* A smaller number of principal components may result in a more interpretable reduced-dimensional representation of the data, as it captures the most important underlying patterns and relationships. This can facilitate easier analysis and understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea46a8-6ab5-4f90-9b08-04a99b18a9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff88a30d-10ab-44d4-b197-365e4f3cdf4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff62bc-d1da-454d-9812-abd857039717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d636e37-bdfb-4e6d-babe-d18dd6f42403",
   "metadata": {},
   "source": [
    "## Question - 5\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92de008-d20e-4feb-b85c-eed6b1113905",
   "metadata": {},
   "source": [
    "PCA can be used for feature selection by leveraging the information captured in the principal components to identify and retain the most important features in a dataset. Here's how PCA can be used for feature selection and its benefits:\n",
    "\n",
    "1. Variance Explanation:\n",
    "\n",
    "* PCA identifies the directions (principal components) in the feature space that capture the most significant variations in the data. Each principal component represents a linear combination of the original features, with coefficients indicating their importance.\n",
    "\n",
    "* By analyzing the coefficients of the principal components, you can identify the features that contribute most to each principal component. Features with larger coefficients are considered more important in explaining the variance in the data.\n",
    "\n",
    "2. Feature Ranking:\n",
    "\n",
    "* After performing PCA, you can rank the original features based on their contributions to the principal components. Features with higher contributions to the principal components are deemed more important and can be selected for inclusion in the reduced-dimensional representation of the data.\n",
    "\n",
    "* You can choose a subset of the top-ranked features to retain, effectively performing feature selection based on their importance in capturing the underlying structure of the data.\n",
    "\n",
    "3. Dimensionality Reduction:\n",
    "\n",
    "* PCA inherently performs dimensionality reduction by selecting a subset of principal components that capture a desired amount of variance in the data. The reduced-dimensional representation of the data contains the most important features that explain the majority of the variance.\n",
    "\n",
    "* By selecting a subset of principal components (and their corresponding features), you effectively perform feature selection and retain only the most informative features in the reduced-dimensional representation.\n",
    "\n",
    "\n",
    "# Benefits:\n",
    "\n",
    "1. Simplicity: PCA provides a simple and straightforward method for feature selection by leveraging the information captured in the principal components. It does not require complex algorithms or extensive parameter tuning.\n",
    "\n",
    "2. Multicollinearity Handling: PCA can handle multicollinearity among features by transforming them into orthogonal principal components, which capture independent sources of variation in the data. This can help mitigate the issue of multicollinearity and improve the stability and interpretability of the selected features.\n",
    "\n",
    "3. Dimensionality Reduction: In addition to feature selection, PCA also performs dimensionality reduction, which can lead to simpler models, improved computational efficiency, and enhanced interpretability.\n",
    "\n",
    "4. Unsupervised Learning: PCA is an unsupervised learning technique, meaning it does not require labeled data for feature selection. It can be applied to both supervised and unsupervised learning tasks, making it versatile and widely applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3457d-17d8-4a15-90fd-758bb8f0c40b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d048c-0cfa-4cd9-8901-709a769111e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da624222-5981-401e-9b1e-1b7799d7a189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca9ed1ee-322c-44e8-ab4a-a4e3ee861aee",
   "metadata": {},
   "source": [
    "# Question - 6\n",
    "ans- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf9c24a-c65b-47c1-8246-f5214b911ded",
   "metadata": {},
   "source": [
    "1. Dimensionality Reduction: PCA is primarily used for dimensionality reduction by transforming high-dimensional data into a lower-dimensional subspace while preserving the most important information. This is useful for data visualization, computational efficiency, and dealing with the curse of dimensionality.\n",
    "\n",
    "2. Feature Extraction: PCA can be used for feature extraction by identifying the most informative features in a dataset and representing them as linear combinations of the original features. This is beneficial for simplifying complex datasets and improving model performance.\n",
    "\n",
    "3. Data Visualization: PCA is often used for data visualization by reducing the dimensionality of data to two or three dimensions, which can be easily visualized in scatter plots or other graphical representations. This helps in exploring and understanding the underlying structure of the data.\n",
    "\n",
    "4. Noise Reduction: PCA can help reduce noise in data by focusing on the principal components that capture the most significant variations in the dataset. By discarding less important components, PCA can enhance signal-to-noise ratios and improve the quality of data analysis.\n",
    "\n",
    "5. Clustering and Classification: PCA can be used as a preprocessing step for clustering and classification tasks. By reducing the dimensionality of the feature space, PCA can improve the performance of clustering algorithms and simplify the classification process.\n",
    "\n",
    "6. Anomaly Detection: PCA can be applied to detect anomalies or outliers in datasets by identifying observations that deviate significantly from the norm in the reduced-dimensional space. This is useful for identifying unusual patterns or behaviors in data.\n",
    "\n",
    "7. Image Processing: In computer vision and image processing, PCA is used for tasks such as face recognition, image compression, and feature extraction. PCA can help reduce the dimensionality of image data while preserving important visual information.\n",
    "\n",
    "8. Signal Processing: PCA finds applications in signal processing for denoising, feature extraction, and dimensionality reduction in time series data or sensor measurements.\n",
    "\n",
    "9. Bioinformatics: In bioinformatics, PCA is used for analyzing gene expression data, protein sequence analysis, and other biological datasets to identify patterns and relationships among variables.\n",
    "\n",
    "10. Finance: PCA is applied in finance for portfolio optimization, risk management, and asset pricing models. By reducing the dimensionality of financial datasets, PCA can help identify latent factors and improve investment strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a1a46d-1352-453c-a483-9b4eec85c906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13060b7b-be5c-482f-aa77-8cf7d504c60a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd445c-24c9-455d-9b66-1fa14177c07c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4395a043-2194-4625-9048-3a7ba047b53c",
   "metadata": {},
   "source": [
    "## Question - 7\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e456ec25-a179-4b6e-89f2-4cea66a7c03f",
   "metadata": {},
   "source": [
    "# 1 Variance:\n",
    "\n",
    "* In PCA, variance represents the amount of information or variability captured by each principal component.\n",
    "\n",
    "* The variance of each principal component is calculated as the eigenvalue associated with that component. Eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "* Principal components with higher eigenvalues (and thus higher variance) capture more information about the underlying structure of the data and contribute more to the overall spread of the data points.\n",
    "\n",
    "\n",
    "# 2 Spread:\n",
    "\n",
    "* Spread refers to the distribution of data points along the principal components in the reduced-dimensional space.\n",
    "\n",
    "* Higher variance (larger eigenvalues) in a principal component corresponds to a greater spread of data points along that component.\n",
    "\n",
    "* Principal components with higher variance capture more variability in the data and result in a wider spread of data points along their directions.\n",
    "\n",
    "# Relationship:\n",
    "\n",
    "* Variance and spread are directly related in PCA. Principal components with higher variance capture more variability in the data, leading to a wider spread of data points along their directions.\n",
    "\n",
    "* Conversely, principal components with lower variance capture less variability in the data and result in a narrower spread of data points along their directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a37a5a-6244-4f77-ae28-4a367c536e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77fe960-375a-4922-bcd8-dcf2aef64d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b1c65-644e-49b5-90df-75a7d8b242c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26c1f82f-14ce-4bc8-b473-d6055caf228b",
   "metadata": {},
   "source": [
    "# Question - 8\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3829664-2496-4e7d-a670-950ce1d48e23",
   "metadata": {},
   "source": [
    "1. Covariance Matrix:\n",
    "\n",
    "PCA begins by computing the covariance matrix of the original data. The covariance matrix quantifies the relationships between pairs of features in the dataset and provides information about the spread and variance of the data along different dimensions.\n",
    "\n",
    "\n",
    "2. Eigenvalue Decomposition:\n",
    "\n",
    "The next step involves performing eigenvalue decomposition (or singular value decomposition) on the covariance matrix. This decomposition yields the eigenvalues and eigenvectors of the covariance matrix.\n",
    "Eigenvalues represent the amount of variance explained by each principal component, while eigenvectors represent the directions (axes) along which the data varies the most.\n",
    "\n",
    "3. Principal Components:\n",
    "\n",
    "The principal components are identified based on the eigenvectors of the covariance matrix. Each eigenvector corresponds to a principal component, and the direction of the eigenvector represents the direction in which the data varies the most.\n",
    "Eigenvectors with higher eigenvalues (larger variance) capture more variability in the data and are deemed more important as principal components.\n",
    "\n",
    "\n",
    "4. Ranking by Variance:\n",
    "\n",
    "The eigenvalues are typically sorted in descending order, and their corresponding eigenvectors (principal components) are arranged accordingly. This ranking allows PCA to prioritize the principal components that capture the most variance in the data.\n",
    "\n",
    "\n",
    "5. Dimensionality Reduction:\n",
    "\n",
    "PCA selects a subset of the top-ranked principal components based on their corresponding eigenvalues and the desired amount of variance to be preserved. This subset forms a new basis for the data onto which the original high-dimensional data is projected.\n",
    "By retaining only the most important principal components, PCA achieves dimensionality reduction while preserving as much information as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fead83-dff0-44df-9e58-fb9377098f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e1515-75ea-43f2-87d9-3e4995a7e1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a50ecc4-4cb7-47f9-9cf9-d8ad35b3d468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76a45288-6fdb-4ae0-a46d-2c9292d2cdfe",
   "metadata": {},
   "source": [
    "## Question - 9\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c07cda-6fe8-4ed9-9b2f-9dfc1738f2da",
   "metadata": {},
   "source": [
    "1. Normalization:\n",
    "\n",
    "* Before performing PCA, it's common practice to normalize or standardize the data to ensure that all dimensions have comparable scales. This prevents dimensions with larger variances from dominating the principal components solely due to their larger magnitude.\n",
    "\n",
    "* Normalization techniques such as z-score standardization (subtracting the mean and dividing by the standard deviation) or min-max scaling (scaling to a predefined range) can be applied to ensure that all dimensions contribute proportionally to the calculation of principal components.\n",
    "\n",
    "2. Covariance Matrix:\n",
    "\n",
    "* PCA computes the covariance matrix of the normalized data. The covariance matrix captures the relationships between pairs of features and provides information about the spread and variance of the data in each dimension.\n",
    "\n",
    "* The covariance matrix allows PCA to identify the directions (principal components) along which the data varies the most, regardless of the absolute scale of the variances in individual dimensions.\n",
    "\n",
    "3. Eigenvalue Decomposition:\n",
    "\n",
    "* PCA performs eigenvalue decomposition (or singular value decomposition) on the covariance matrix to obtain the eigenvalues and eigenvectors. Eigenvalues represent the amount of variance explained by each principal component, while eigenvectors represent the directions of maximum variance.\n",
    "\n",
    "* Eigenvectors with higher eigenvalues capture more variability in the data and are prioritized as principal components, regardless of the absolute scale of the variances in individual dimensions.\n",
    "\n",
    "4. Principal Components Selection:\n",
    "\n",
    "* PCA selects a subset of the principal components based on their corresponding eigenvalues and the desired amount of variance to be preserved. Principal components with higher eigenvalues (and thus capturing more variance) are retained, while those with lower eigenvalues are discarded.\n",
    "\n",
    "* By focusing on the principal components that capture the most significant variability in the data, PCA effectively handles situations where certain dimensions have high variance while others have low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9346f7b-32a2-4cce-8e05-27f2cdf44127",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
